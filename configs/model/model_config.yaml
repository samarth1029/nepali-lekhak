llm_model:
  vocab_size: 50000
  context_length: 256
  emb_dim: 768
  n_heads: 12
  n_layers: 12
  drop_rate: 0.1
  qkv_bias: false