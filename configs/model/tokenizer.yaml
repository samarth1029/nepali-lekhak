tokenizer:
  vocab_size: 50000
  min_frequency: 2
  special_tokens:
    - "<PAD>"
    - "<UNK>"
    - "<BOS>"
    - "<EOS>"
    - "<|endoftext|>"
  save_dir: "artifacts/tokenizer"
  vocab_file: "vocab1k.json"
  merges_file: "bpe_merges.txt"
